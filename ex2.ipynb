{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# pytorch mlp for multiclass classification\nfrom numpy import vstack\nfrom numpy import argmax\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import read_csv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom torch import Tensor\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom torch.nn import Linear\nfrom torch.nn import ReLU\nfrom torch.nn import Softmax\nfrom torch.nn import Module\nfrom torch.optim import Adam\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.init import kaiming_uniform_\nfrom torch.nn.init import xavier_uniform_\nfrom tqdm import tqdm\nfrom torch.nn import Dropout","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-12T19:42:16.437616Z","iopub.execute_input":"2023-11-12T19:42:16.438027Z","iopub.status.idle":"2023-11-12T19:42:16.446201Z","shell.execute_reply.started":"2023-11-12T19:42:16.437997Z","shell.execute_reply":"2023-11-12T19:42:16.445248Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"path = pd.read_csv('/kaggle/input/machine-predictive-maintenance-classification/predictive_maintenance.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.570330Z","iopub.status.idle":"2023-11-12T19:41:58.570846Z","shell.execute_reply.started":"2023-11-12T19:41:58.570622Z","shell.execute_reply":"2023-11-12T19:41:58.570645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.572479Z","iopub.status.idle":"2023-11-12T19:41:58.572901Z","shell.execute_reply.started":"2023-11-12T19:41:58.572705Z","shell.execute_reply":"2023-11-12T19:41:58.572725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(path['Type'].unique())  #les valeurs de symbole\nprint(path['Target'].unique())  #les valeurs de symbole\nprint(path['Failure Type'].unique())  #les valeurs de symbole\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.574288Z","iopub.status.idle":"2023-11-12T19:41:58.574674Z","shell.execute_reply.started":"2023-11-12T19:41:58.574493Z","shell.execute_reply":"2023-11-12T19:41:58.574511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display basic information about the dataset\nprint(path.info())\n\n# Display summary statistics\nprint(path.describe())","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.576022Z","iopub.status.idle":"2023-11-12T19:41:58.576415Z","shell.execute_reply.started":"2023-11-12T19:41:58.576233Z","shell.execute_reply":"2023-11-12T19:41:58.576252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values\nprint(path.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.577674Z","iopub.status.idle":"2023-11-12T19:41:58.578055Z","shell.execute_reply.started":"2023-11-12T19:41:58.577877Z","shell.execute_reply":"2023-11-12T19:41:58.577895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot histograms for numeric columns\npath.hist(bins=20, figsize=(15,10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.579821Z","iopub.status.idle":"2023-11-12T19:41:58.580495Z","shell.execute_reply.started":"2023-11-12T19:41:58.580300Z","shell.execute_reply":"2023-11-12T19:41:58.580321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset definition\nclass CSVDataset(Dataset):\n    # load the dataset\n    def __init__(self, path):\n        # load the csv file as a dataframe\n        df = read_csv(path, header=None , low_memory=False)\n        # store the inputs and outputs\n        self.X = df.values[1:, 3:8]\n        self.y = df.values[1:, -1]\n        # ensure input data is floats\n        self.X = self.X.astype('float64')\n        # label encode target and ensure the values are floats\n        self.y = LabelEncoder().fit_transform(self.y)\n        \n    # number of rows in the dataset\n    def __len__(self):\n        return len(self.X)\n\n    # get a row at an index\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n\n    # get indexes for train and test rows\n    def get_splits(self, n_test=0.33):\n        # determine sizes\n        test_size = round(n_test * len(self.X))\n        train_size = len(self.X) - test_size\n        # calculate the split\n        return random_split(self, [train_size, test_size])","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.581622Z","iopub.status.idle":"2023-11-12T19:41:58.581999Z","shell.execute_reply.started":"2023-11-12T19:41:58.581819Z","shell.execute_reply":"2023-11-12T19:41:58.581838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn import Module, Linear, ReLU, Softmax, Dropout\nfrom torch.nn.init import kaiming_uniform_, xavier_uniform_\n\nclass MLP(Module):\n    def __init__(self, n_inputs):\n        super(MLP, self).__init__()\n        # input to the first hidden layer\n        self.hidden1 = Linear(n_inputs, 10)\n        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n        self.act1 = ReLU()\n        self.dropout1 = Dropout(0.2)\n        \n        # second hidden layer\n        self.hidden2 = Linear(10, 8)\n        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n        self.act2 = ReLU()\n        \n        # third hidden layer and output\n        self.hidden3 = Linear(8, 2)\n        xavier_uniform_(self.hidden3.weight)\n        self.act3 = Softmax(dim=1)\n\n    def forward(self, X):\n        X = self.hidden1(X)\n        X = self.act1(X)\n        X = self.dropout1(X) \n        \n        X = self.hidden2(X)\n        X = self.act2(X)\n        \n        X = self.hidden3(X)\n        X = self.act3(X)\n        return X","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.583345Z","iopub.status.idle":"2023-11-12T19:41:58.583708Z","shell.execute_reply.started":"2023-11-12T19:41:58.583532Z","shell.execute_reply":"2023-11-12T19:41:58.583549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare the dataset\ndef prepare_data(path):\n    # load the dataset\n    dataset = CSVDataset(path)\n    # calculate split\n    train, test = dataset.get_splits()\n    # prepare data loaders\n    train_dl = DataLoader(train, batch_size=1024, shuffle=True)\n    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n    return train_dl, test_dl","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.601898Z","iopub.execute_input":"2023-11-12T19:41:58.603291Z","iopub.status.idle":"2023-11-12T19:41:58.610290Z","shell.execute_reply.started":"2023-11-12T19:41:58.603223Z","shell.execute_reply":"2023-11-12T19:41:58.609246Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print            \n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.612681Z","iopub.execute_input":"2023-11-12T19:41:58.613101Z","iopub.status.idle":"2023-11-12T19:41:58.629832Z","shell.execute_reply.started":"2023-11-12T19:41:58.613065Z","shell.execute_reply":"2023-11-12T19:41:58.628761Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"number_epochs = 100 \nlearning_rate = 0.01\nloss_per_epoch = []\nloss_per_epoch_validation= []\n# train the model\ndef train_model(train_dl, model):\n    #desired_error = 10  # Set your desired error level\n    #average_loss = 200\n    # early stop patience\n    patience = 7\n    size = len(train_dl.dataset)\n    # define loss function MSE \n    criterion = CrossEntropyLoss()\n    # define the optimization ADAM \n    # regularization L2 = weight_decay\n    optimizer =  Adam(model.parameters(), lr=learning_rate , weight_decay= 0.01)\n    epoch = 0 \n    # Early stop object\n    early_stopping = EarlyStopping(patience=patience, verbose=True)\n    # enumerate epochs\n    #while(average_loss >= desired_error): # accroding to loss function do epochs \n    for epoch in tqdm(range(number_epochs),desc='Training Epochs'):\n        print(f\"Epoch {epoch+1}\\n-------------------------------\") # enumerate mini batches\n        # training data \n        total_loss = 0.0\n        model.train()\n        TrainigStep_loss = []\n        for batch, (inputs, targets) in enumerate(train_dl):\n            \n            # clear the gradients\n            optimizer.zero_grad()\n            # compute the model output\n            yhat = model(inputs)\n            #print(\"y befor\" , inputs)\n            # calculate loss\n            loss = criterion(yhat, targets)\n            # credit assignment\n            loss.backward()\n            # update model weights\n            optimizer.step()\n            TrainigStep_loss.append(loss.item())\n            #total_loss += loss.item()\n        \n        #average_loss = total_loss / len(train_dl)\n        loss = np.array(TrainigStep_loss).mean()\n        loss_per_epoch.append(loss)\n        print(f\"loss: {loss:>7f}\")\n        #print(f\"Average loss :{average_loss:>7f}\" )\n        \n        # test / validation data \n        model.eval()     # Optional when not using Model Specific layer\n        validationStep_loss = []\n        for batch, (inputs, targets) in enumerate(test_dl):\n            \n            # Forward Pass\n            outputs = model(inputs)\n            # Find the Loss\n            validation_loss = criterion(outputs, targets)\n            # Calculate Loss\n            validationStep_loss.append(validation_loss.item())\n            \n        loss_per_epoch_validation.append(np.array(validationStep_loss).mean())\n        \n        # print training/validation statistics \n        # calculate average loss over an epoch\n        train_loss = np.average(TrainigStep_loss)\n        valid_loss = np.average(validationStep_loss)\n        \n        # early stopping\n        early_stopping(valid_loss, model)\n        \n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n        '''\n        early_stopper = EarlyStopper(patience=3, min_delta=10)\n        if early_stopper.early_stop(np.array(loss_per_epoch_validation).mean()):  \n            print(\"Early stop lunched \" ,epoch )\n            break\n        '''\n        #epoch+=1\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.632078Z","iopub.execute_input":"2023-11-12T19:41:58.632651Z","iopub.status.idle":"2023-11-12T19:41:58.646116Z","shell.execute_reply.started":"2023-11-12T19:41:58.632612Z","shell.execute_reply":"2023-11-12T19:41:58.644972Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# evaluate the model\ndef evaluate_model(test_dl, model):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_dl):\n        # evaluate the model on the test set\n        yhat = model(inputs)\n        # retrieve numpy array\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy()\n        # convert to class labels\n        yhat = argmax(yhat, axis=1)\n        # reshape for stacking\n        actual = actual.reshape((len(actual), 1))\n        yhat = yhat.reshape((len(yhat), 1))\n        # store\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    # calculate accuracy\n    acc = accuracy_score(actuals, predictions)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.647326Z","iopub.execute_input":"2023-11-12T19:41:58.647732Z","iopub.status.idle":"2023-11-12T19:41:58.662558Z","shell.execute_reply.started":"2023-11-12T19:41:58.647697Z","shell.execute_reply":"2023-11-12T19:41:58.661123Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# make a class prediction for one row of data\ndef predict(row, model):\n    # convert row to data\n    row = Tensor([row])\n    # make prediction\n    yhat = model(row)\n    # retrieve numpy array\n    yhat = yhat.detach().numpy()\n    return yhat\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T19:41:58.666709Z","iopub.execute_input":"2023-11-12T19:41:58.667848Z","iopub.status.idle":"2023-11-12T19:41:58.677999Z","shell.execute_reply.started":"2023-11-12T19:41:58.667800Z","shell.execute_reply":"2023-11-12T19:41:58.676953Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# prepare the data\npath = '/kaggle/input/machine-predictive-maintenance-classification/predictive_maintenance.csv'\ntrain_dl, test_dl = prepare_data(path)\nprint(len(train_dl.dataset), len(test_dl.dataset))\n# define the network\nmodel = MLP(5)\n# train the model\ntrain_model(train_dl, model)","metadata":{},"execution_count":null,"outputs":[]}]}